{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35225ddc-3c1a-403f-8b28-828720c024e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# TFT multivariate experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac85e43-5d83-4128-99dc-699db8be4745",
   "metadata": {},
   "source": [
    "This notebook contains the experiments for TFT on on the three multivariate datasets.\n",
    "1. Contrary, to Informer, Autoformer and DLinear implementation, we manually need to scale and split the datasets.\n",
    "2. Add time features such as month of the year, day of week etc..\n",
    "3. Implement an experiment_main() method that runs the whole model training across datasets and prediction lengths\n",
    "4. Wrap the data in the required format by pytorch-forecasting and set up the model\n",
    "5. We calculate the results with a rolling test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8fedff-1c00-48b2-b512-760d2acfe726",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import EncoderNormalizer\n",
    "from pytorch_forecasting.data import MultiNormalizer\n",
    "from pytorch_forecasting.data import TorchNormalizer\n",
    "from pytorch_forecasting.metrics import MAE, RMSE, SMAPE, PoissonLoss, QuantileLoss\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39032213-9db1-4577-879d-1a82b1a50e5f",
   "metadata": {},
   "source": [
    "## 1. Load, Scale, Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2750984-4e55-4bd9-9bc5-3c2ca90aba28",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../../01_datasets/df_all_columns.csv'\n",
    "df_all_columns = pd.read_csv(file_path)\n",
    "\n",
    "file_path = '../../01_datasets/df_most_important_columns.csv'\n",
    "df_most_important_columns = pd.read_csv(file_path)\n",
    "\n",
    "file_path = '../../01_datasets/df_only_generation_columns.csv'\n",
    "df_only_generation_columns = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01eb67c1-964b-4a17-8492-84daa700c2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    \n",
    "    # Extract the date column\n",
    "    dates = df['date']\n",
    "    \n",
    "    # Exclude the date column for scaling\n",
    "    df = df.drop(columns=['date'])\n",
    "    \n",
    "    # Define the sizes for training, validation, and test sets (70%,10%,20%)\n",
    "    train_size = int(len(df) * 0.7)\n",
    "    val_size = int((len(df) * 0.1)+1)\n",
    "    test_size = len(df) - train_size - val_size\n",
    "    \n",
    "    # Split the data into training, validation, and test set\n",
    "    train_data = df.iloc[:train_size]\n",
    "    val_data = df.iloc[train_size:train_size + val_size]\n",
    "    test_data = df.iloc[train_size + val_size:]\n",
    "    \n",
    "    # Initialize the StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Fit the scaler on the training data\n",
    "    scaler.fit(train_data)\n",
    "    \n",
    "    # Transform the datasets using the same scaler\n",
    "    train_standardized = scaler.transform(train_data)\n",
    "    val_standardized = scaler.transform(val_data)\n",
    "    test_standardized = scaler.transform(test_data)\n",
    "    \n",
    "    # Create new DataFrames with standardized values, including the date column\n",
    "    train_data = pd.DataFrame(train_standardized, columns=train_data.columns)\n",
    "    val_data = pd.DataFrame(val_standardized, columns=df.columns)\n",
    "    test_data = pd.DataFrame(test_standardized, columns=test_data.columns)\n",
    "    \n",
    "    # Add the date column back to the data\n",
    "    train_data['date'] = dates.iloc[:train_size].values\n",
    "    val_data['date'] = dates.iloc[train_size:train_size + val_size].values\n",
    "    test_data['date'] = dates.iloc[train_size + val_size:].values\n",
    "     \n",
    "    # Set 'date' column as index and convert it to datetime format\n",
    "    train_data['date'] = pd.to_datetime(train_data['date'])\n",
    "    train_data.set_index('date', inplace=True)\n",
    "\n",
    "    test_data['date'] = pd.to_datetime(test_data['date'])\n",
    "    test_data.set_index('date', inplace=True)\n",
    "    \n",
    "    val_data['date'] = pd.to_datetime(val_data['date'])\n",
    "    val_data.set_index('date', inplace=True)\n",
    " \n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4717c32-ee9a-491a-bed0-a4118b8d9f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_all_columns, val_data_all_columns, test_data_all_columns = preprocess_data(df_all_columns)\n",
    "train_data_most_important_columns, val_data_most_important_columns, test_data_most_important_columns = preprocess_data(df_most_important_columns)\n",
    "train_data_only_generation_columns, val_data_only_generation_columns, test_data_only_generation_columns = preprocess_data(df_only_generation_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d73cf47a-7e19-436c-a546-c2e0a9fca2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is needed because TFT requires one dataset as an input and additional information on where to split up between training and validation\n",
    "data_all_columns = pd.concat([train_data_all_columns, val_data_all_columns], ignore_index=False)\n",
    "data_most_important_columns = pd.concat([train_data_most_important_columns, val_data_most_important_columns], ignore_index=False)\n",
    "data_only_generation_columns = pd.concat([train_data_only_generation_columns, val_data_only_generation_columns], ignore_index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea0ed55-084f-4f3b-ab91-0fa816e6fb65",
   "metadata": {},
   "source": [
    "## 2. Time Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bfa7bda-f1a2-4d63-8ddb-5a268538aaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_features(df):\n",
    "    # Convert the index to a DateTimeIndex if it's not already\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "    \n",
    "    # Extract the desired date components\n",
    "    hour_of_day = df.index.hour.astype(str).astype(\"category\")\n",
    "    day_of_month = df.index.day.astype(str).astype(\"category\")\n",
    "    day_of_year = df.index.dayofyear.astype(str).astype(\"category\")\n",
    "    month_of_year = df.index.month.astype(str).astype(\"category\")\n",
    "    week_of_year = df.index.isocalendar().week.astype(str).astype(\"category\")\n",
    "    day_of_week = df.index.dayofweek.astype(str).astype(\"category\")  # Monday is 0, Sunday is 6\n",
    "    # Append these Series as new columns in the DataFrame\n",
    "    df = df.assign(\n",
    "        hour_of_day=hour_of_day,\n",
    "        day_of_month=day_of_month,\n",
    "        day_of_year=day_of_year,\n",
    "        month_of_year=month_of_year,\n",
    "        week_of_year=week_of_year,\n",
    "        day_of_week=day_of_week\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dbe8c65-da8f-4bc6-b1b8-a18d390cfcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all_columns = add_time_features(data_all_columns)\n",
    "data_most_important_columns = add_time_features(data_most_important_columns)\n",
    "data_only_generation_columns = add_time_features(data_only_generation_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e09d6b07-ee4a-4dc8-a62a-a55f40de4f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index for data_all_columns\n",
    "# A time_idx is required by TFT\n",
    "# TFT requires at least on category for prediction. Since we only have one category/one country we add 'DE' to every row\n",
    "data_all_columns = data_all_columns.reset_index(drop=True)\n",
    "data_all_columns['time_idx']=data_all_columns.index\n",
    "data_all_columns['DE'] = 'DE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c0085c4-e1e3-4f96-84c5-6474b6dd985a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_most_important_columns = data_most_important_columns.reset_index(drop=True)\n",
    "data_most_important_columns['time_idx']= data_most_important_columns.index\n",
    "data_most_important_columns['DE'] = 'DE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d586fce-bf72-480e-a0c2-aaa9e5c11de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_only_generation_columns = data_only_generation_columns.reset_index(drop=True)\n",
    "data_only_generation_columns['time_idx'] = data_only_generation_columns.index\n",
    "data_only_generation_columns['DE'] = 'DE'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0420c924-e03c-431e-bd61-47ba7b348690",
   "metadata": {},
   "source": [
    "## 3. Experiment Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b062705-70f7-49a3-bd45-ef77d298e4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_main():\n",
    "    # Define the save directory\n",
    "    save_dir = 'TFT_Results' \n",
    "    \n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    datasets = [data_all_columns, data_most_important_columns, data_only_generation_columns]\n",
    "    prediction_lengths = [24, 48, 96, 192]\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        dataset_name = \"data_all_columns\" if dataset is data_all_columns else \"data_most_important_columns\" if dataset is data_most_important_columns else \"data_only_generation_columns\"\n",
    "        data = dataset\n",
    "        # List of columns\n",
    "        columns = [col for col in data.columns if col not in ['DE', 'time_idx','hour_of_day','day_of_month','day_of_year','month_of_year','week_of_year','day_of_week']]\n",
    "        print(dataset_name)\n",
    "        for pred_len in prediction_lengths:\n",
    "            dataset_builder(data, pred_len, columns)\n",
    "\n",
    "            #network_configuration(dataset)\n",
    "            #training()\n",
    "            print(pred_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64d459a7-6db9-42c7-8165-23b938f56037",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_builder(data, pred_len, columns):\n",
    "    \n",
    "    # Sets the maximum encoder length same across models\n",
    "    max_encoder_length = 96\n",
    "    # Ensures the validation data is not used for training\n",
    "    training_cutoff = data[\"time_idx\"].max() - 4379\n",
    "    # Create a list of target normalizers for each column\n",
    "    target_normalizers = [TorchNormalizer(method='identity', center=False, transformation=None, method_kwargs={}) for _ in range(len(columns))]\n",
    "    \n",
    "    # Define the training dataset in the TimeSeriesDataSet format\n",
    "    training = TimeSeriesDataSet(\n",
    "        data[lambda x: x.time_idx < training_cutoff],\n",
    "        time_idx=\"time_idx\",\n",
    "        target=columns,\n",
    "        group_ids=[\"DE\"],\n",
    "        min_encoder_length=max_encoder_length,  # keep encoder length long (as it is in the validation set)\n",
    "        max_encoder_length=max_encoder_length,\n",
    "        min_prediction_length=pred_len,\n",
    "        max_prediction_length=pred_len,\n",
    "        static_categoricals=[\"DE\"],\n",
    "        time_varying_known_categoricals=['hour_of_day','day_of_month','day_of_year','month_of_year','week_of_year','day_of_week'],\n",
    "        time_varying_known_reals=[\"time_idx\"],\n",
    "        time_varying_unknown_categoricals=[],\n",
    "        time_varying_unknown_reals=columns,\n",
    "        # Pytorch-forecasting requires a target_normalizer. However, the data is already normalized to use the same normalization across models. Therefore, this normalizer does not change the data.\n",
    "        target_normalizer=MultiNormalizer(target_normalizers),\n",
    "        add_relative_time_idx=True,\n",
    "        add_target_scales=True,\n",
    "        add_encoder_length=True,\n",
    "    )\n",
    "    print(training)\n",
    "    # Define validation dataset with rolling validation approach\n",
    "    validation = TimeSeriesDataSet.from_dataset(training, data, predict=False, min_prediction_idx=data[\"time_idx\"].max() - 4379, predict_mode=False, stop_randomization=True)\n",
    "    print(validation)\n",
    "    \n",
    "    # Create dataloaders for model\n",
    "    batch_size = 128  # set this between 32 to 128\n",
    "    train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "    val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=0)\n",
    "    \n",
    "    # Configure network and trainer\n",
    "    pl.seed_everything(42)\n",
    "    early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
    "    lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "    logger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=50,\n",
    "        accelerator=\"gpu\",\n",
    "        enable_model_summary=True,\n",
    "        gradient_clip_val=0.1,\n",
    "        limit_train_batches=50,  # coment in for training, running valiation every 30 batches\n",
    "        # fast_dev_run=True,  # comment in to check that networkor dataset has no serious bugs\n",
    "        callbacks=[lr_logger, early_stop_callback],\n",
    "        logger=logger,\n",
    "    )\n",
    "\n",
    "    tft = TemporalFusionTransformer.from_dataset(\n",
    "        training,\n",
    "        learning_rate=0.03,\n",
    "        hidden_size=16,\n",
    "        attention_head_size=2,\n",
    "        dropout=0.1,\n",
    "        hidden_continuous_size=8,\n",
    "        loss=MAE(),\n",
    "        log_interval=10,  # uncomment for learning rate finder and otherwise, e.g. to 10 for logging every 10 batches\n",
    "        optimizer=\"adam\",\n",
    "        reduce_on_plateau_patience=4,\n",
    "    )\n",
    "    print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")\n",
    "    \n",
    "     # Fit network\n",
    "    trainer.fit(\n",
    "        tft,\n",
    "        train_dataloaders=train_dataloader,\n",
    "        val_dataloaders=val_dataloader,\n",
    "    )\n",
    "    \n",
    "    # Define the folder where you want to save the information\n",
    "    output_folder = \"TFT_Results\"\n",
    "\n",
    "    # Define the file path within the folder\n",
    "    output_file = os.path.join(output_folder, \"model_info_multivariate.txt\")\n",
    "\n",
    "    # Create a dictionary to store data\n",
    "    data = {\"pred_len\": pred_len, \"num_columns\": len(columns), \"best_model_path\": trainer.checkpoint_callback.best_model_path}\n",
    "    \n",
    "    # Save the data to the file\n",
    "    with open(output_file, \"a\") as file:\n",
    "        file.write(str(data) + \"\\n\")\n",
    "        \n",
    "    # Print the saved information\n",
    "    print(\"Model information saved to\", output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd9f795d-3957-41fc-9533-ab6afb632446",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_all_columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeSeriesDataSet[length=30368](\n",
      "\ttime_idx='time_idx',\n",
      "\ttarget=['DE_load_actual_entsoe_transparency', 'DE_solar_capacity', 'DE_solar_generation_actual', 'DE_solar_profile', 'DE_wind_capacity', 'DE_wind_generation_actual', 'DE_wind_profile', 'DE_wind_offshore_capacity', 'DE_wind_offshore_generation_actual', 'DE_wind_offshore_profile', 'DE_wind_onshore_capacity', 'DE_wind_onshore_generation_actual', 'DE_wind_onshore_profile', 'DE_50hertz_load_actual_entsoe_transparency', 'DE_50hertz_solar_generation_actual', 'DE_50hertz_wind_generation_actual', 'DE_50hertz_wind_offshore_generation_actual', 'DE_50hertz_wind_onshore_generation_actual', 'DE_amprion_load_actual_entsoe_transparency', 'DE_amprion_solar_generation_actual', 'DE_amprion_wind_onshore_generation_actual', 'DE_tennet_load_actual_entsoe_transparency', 'DE_tennet_solar_generation_actual', 'DE_tennet_wind_generation_actual', 'DE_tennet_wind_offshore_generation_actual', 'DE_tennet_wind_onshore_generation_actual', 'DE_transnetbw_load_actual_entsoe_transparency', 'DE_transnetbw_solar_generation_actual', 'DE_transnetbw_wind_onshore_generation_actual'],\n",
      "\tgroup_ids=['DE'],\n",
      "\tweight=None,\n",
      "\tmax_encoder_length=96,\n",
      "\tmin_encoder_length=96,\n",
      "\tmin_prediction_idx=0,\n",
      "\tmin_prediction_length=192,\n",
      "\tmax_prediction_length=192,\n",
      "\tstatic_categoricals=['DE'],\n",
      "\tstatic_reals=['encoder_length', 'DE_load_actual_entsoe_transparency_center', 'DE_load_actual_entsoe_transparency_scale', 'DE_solar_capacity_center', 'DE_solar_capacity_scale', 'DE_solar_generation_actual_center', 'DE_solar_generation_actual_scale', 'DE_solar_profile_center', 'DE_solar_profile_scale', 'DE_wind_capacity_center', 'DE_wind_capacity_scale', 'DE_wind_generation_actual_center', 'DE_wind_generation_actual_scale', 'DE_wind_profile_center', 'DE_wind_profile_scale', 'DE_wind_offshore_capacity_center', 'DE_wind_offshore_capacity_scale', 'DE_wind_offshore_generation_actual_center', 'DE_wind_offshore_generation_actual_scale', 'DE_wind_offshore_profile_center', 'DE_wind_offshore_profile_scale', 'DE_wind_onshore_capacity_center', 'DE_wind_onshore_capacity_scale', 'DE_wind_onshore_generation_actual_center', 'DE_wind_onshore_generation_actual_scale', 'DE_wind_onshore_profile_center', 'DE_wind_onshore_profile_scale', 'DE_50hertz_load_actual_entsoe_transparency_center', 'DE_50hertz_load_actual_entsoe_transparency_scale', 'DE_50hertz_solar_generation_actual_center', 'DE_50hertz_solar_generation_actual_scale', 'DE_50hertz_wind_generation_actual_center', 'DE_50hertz_wind_generation_actual_scale', 'DE_50hertz_wind_offshore_generation_actual_center', 'DE_50hertz_wind_offshore_generation_actual_scale', 'DE_50hertz_wind_onshore_generation_actual_center', 'DE_50hertz_wind_onshore_generation_actual_scale', 'DE_amprion_load_actual_entsoe_transparency_center', 'DE_amprion_load_actual_entsoe_transparency_scale', 'DE_amprion_solar_generation_actual_center', 'DE_amprion_solar_generation_actual_scale', 'DE_amprion_wind_onshore_generation_actual_center', 'DE_amprion_wind_onshore_generation_actual_scale', 'DE_tennet_load_actual_entsoe_transparency_center', 'DE_tennet_load_actual_entsoe_transparency_scale', 'DE_tennet_solar_generation_actual_center', 'DE_tennet_solar_generation_actual_scale', 'DE_tennet_wind_generation_actual_center', 'DE_tennet_wind_generation_actual_scale', 'DE_tennet_wind_offshore_generation_actual_center', 'DE_tennet_wind_offshore_generation_actual_scale', 'DE_tennet_wind_onshore_generation_actual_center', 'DE_tennet_wind_onshore_generation_actual_scale', 'DE_transnetbw_load_actual_entsoe_transparency_center', 'DE_transnetbw_load_actual_entsoe_transparency_scale', 'DE_transnetbw_solar_generation_actual_center', 'DE_transnetbw_solar_generation_actual_scale', 'DE_transnetbw_wind_onshore_generation_actual_center', 'DE_transnetbw_wind_onshore_generation_actual_scale'],\n",
      "\ttime_varying_known_categoricals=['hour_of_day', 'day_of_month', 'day_of_year', 'month_of_year', 'week_of_year', 'day_of_week'],\n",
      "\ttime_varying_known_reals=['time_idx', 'relative_time_idx'],\n",
      "\ttime_varying_unknown_categoricals=[],\n",
      "\ttime_varying_unknown_reals=['DE_load_actual_entsoe_transparency', 'DE_solar_capacity', 'DE_solar_generation_actual', 'DE_solar_profile', 'DE_wind_capacity', 'DE_wind_generation_actual', 'DE_wind_profile', 'DE_wind_offshore_capacity', 'DE_wind_offshore_generation_actual', 'DE_wind_offshore_profile', 'DE_wind_onshore_capacity', 'DE_wind_onshore_generation_actual', 'DE_wind_onshore_profile', 'DE_50hertz_load_actual_entsoe_transparency', 'DE_50hertz_solar_generation_actual', 'DE_50hertz_wind_generation_actual', 'DE_50hertz_wind_offshore_generation_actual', 'DE_50hertz_wind_onshore_generation_actual', 'DE_amprion_load_actual_entsoe_transparency', 'DE_amprion_solar_generation_actual', 'DE_amprion_wind_onshore_generation_actual', 'DE_tennet_load_actual_entsoe_transparency', 'DE_tennet_solar_generation_actual', 'DE_tennet_wind_generation_actual', 'DE_tennet_wind_offshore_generation_actual', 'DE_tennet_wind_onshore_generation_actual', 'DE_transnetbw_load_actual_entsoe_transparency', 'DE_transnetbw_solar_generation_actual', 'DE_transnetbw_wind_onshore_generation_actual'],\n",
      "\tvariable_groups={},\n",
      "\tconstant_fill_strategy={},\n",
      "\tallow_missing_timesteps=False,\n",
      "\tlags={},\n",
      "\tadd_relative_time_idx=True,\n",
      "\tadd_target_scales=True,\n",
      "\tadd_encoder_length=True,\n",
      "\ttarget_normalizer=MultiNormalizer(\n",
      "\tnormalizers=[TorchNormalizer(method='identity', center=False, transformation=None, method_kwargs={}), TorchNormalizer(method='identity', center=False, transformation=None, method_kwargs={}), TorchNormalizer(method='identity', center=False, transformation=None, method_kwargs={}), TorchNormalizer(method='identity', center=False, transformation=None, method_kwargs={}), TorchNormalizer(method='identity', center=False, transformation=None, method_kwargs={}), TorchNormalizer(method='identity', center=False, transformation=None, method_kwargs={}), TorchNormalizer(method='identity', center=False, transformation=None, method_kwargs={}), TorchNormalizer(method='identity', center=False, transformation=None, method_kwargs={}), TorchNormalizer(method='identity', center=False, transformation=None, method_kwargs={}), TorchNormalizer(method='identity', center=False, transformation=None, method_kwargs={}), TorchNormalizer(method='identity', center=False, transformation=None, method_kwargs={}), TorchNormalizer(method='identity', center=False, transformation=None, method_kwargs={}), TorchNormalizer(method='identity', center=False, transformation=None, method_kwargs={}), TorchNormalizer(method='identity', center=False, transformation=None, method_kwargs={}), TorchNormalizer(method='identity', center=False, transformation=None, method_kwargs={}), TorchNormalizer(method='identity', center=False, transformation=None, method_kwargs={}), TorchNormalizer(method='identity', center=False, transformation=None, method_kwargs={}), TorchNormalizer(method='identity', center=False, transformation=None, method_kwargs={}), TorchNormalizer(method='identity', center=False, transformation=None, method_kwargs={}), TorchNormalizer(method='identity', center=False, transformation=None, method_kwargs={}), TorchNormalizer(method='identity', center=False, transformation=None, method_kwargs={}), TorchNormalizer(method='identity', center=False, transformation=None, method_kwargs={}), TorchNormalizer(method='identity', center=False, transformation=None, method_kwargs={}), TorchNormalizer(method='identity', center=False, transformation=None, method_kwargs={}), TorchNormalizer(method='identity', center=False, transformation=None, method_kwargs={}), TorchNormalizer(method='identity', center=False, transformation=None, method_kwargs={}), TorchNormalizer(method='identity', center=False, transformation=None, method_kwargs={}), TorchNormalizer(method='identity', center=False, transformation=None, method_kwargs={}), TorchNormalizer(method='identity', center=False, transformation=None, method_kwargs={})]\n",
      "),\n",
      "\tcategorical_encoders={'__group_id__DE': NaNLabelEncoder(add_nan=False, warn=True), 'DE': NaNLabelEncoder(add_nan=False, warn=True), 'hour_of_day': NaNLabelEncoder(add_nan=False, warn=True), 'day_of_month': NaNLabelEncoder(add_nan=False, warn=True), 'day_of_year': NaNLabelEncoder(add_nan=False, warn=True), 'month_of_year': NaNLabelEncoder(add_nan=False, warn=True), 'week_of_year': NaNLabelEncoder(add_nan=False, warn=True), 'day_of_week': NaNLabelEncoder(add_nan=False, warn=True)},\n",
      "\tscalers={'encoder_length': StandardScaler(), 'DE_load_actual_entsoe_transparency_center': StandardScaler(), 'DE_load_actual_entsoe_transparency_scale': StandardScaler(), 'DE_solar_capacity_center': StandardScaler(), 'DE_solar_capacity_scale': StandardScaler(), 'DE_solar_generation_actual_center': StandardScaler(), 'DE_solar_generation_actual_scale': StandardScaler(), 'DE_solar_profile_center': StandardScaler(), 'DE_solar_profile_scale': StandardScaler(), 'DE_wind_capacity_center': StandardScaler(), 'DE_wind_capacity_scale': StandardScaler(), 'DE_wind_generation_actual_center': StandardScaler(), 'DE_wind_generation_actual_scale': StandardScaler(), 'DE_wind_profile_center': StandardScaler(), 'DE_wind_profile_scale': StandardScaler(), 'DE_wind_offshore_capacity_center': StandardScaler(), 'DE_wind_offshore_capacity_scale': StandardScaler(), 'DE_wind_offshore_generation_actual_center': StandardScaler(), 'DE_wind_offshore_generation_actual_scale': StandardScaler(), 'DE_wind_offshore_profile_center': StandardScaler(), 'DE_wind_offshore_profile_scale': StandardScaler(), 'DE_wind_onshore_capacity_center': StandardScaler(), 'DE_wind_onshore_capacity_scale': StandardScaler(), 'DE_wind_onshore_generation_actual_center': StandardScaler(), 'DE_wind_onshore_generation_actual_scale': StandardScaler(), 'DE_wind_onshore_profile_center': StandardScaler(), 'DE_wind_onshore_profile_scale': StandardScaler(), 'DE_50hertz_load_actual_entsoe_transparency_center': StandardScaler(), 'DE_50hertz_load_actual_entsoe_transparency_scale': StandardScaler(), 'DE_50hertz_solar_generation_actual_center': StandardScaler(), 'DE_50hertz_solar_generation_actual_scale': StandardScaler(), 'DE_50hertz_wind_generation_actual_center': StandardScaler(), 'DE_50hertz_wind_generation_actual_scale': StandardScaler(), 'DE_50hertz_wind_offshore_generation_actual_center': StandardScaler(), 'DE_50hertz_wind_offshore_generation_actual_scale': StandardScaler(), 'DE_50hertz_wind_onshore_generation_actual_center': StandardScaler(), 'DE_50hertz_wind_onshore_generation_actual_scale': StandardScaler(), 'DE_amprion_load_actual_entsoe_transparency_center': StandardScaler(), 'DE_amprion_load_actual_entsoe_transparency_scale': StandardScaler(), 'DE_amprion_solar_generation_actual_center': StandardScaler(), 'DE_amprion_solar_generation_actual_scale': StandardScaler(), 'DE_amprion_wind_onshore_generation_actual_center': StandardScaler(), 'DE_amprion_wind_onshore_generation_actual_scale': StandardScaler(), 'DE_tennet_load_actual_entsoe_transparency_center': StandardScaler(), 'DE_tennet_load_actual_entsoe_transparency_scale': StandardScaler(), 'DE_tennet_solar_generation_actual_center': StandardScaler(), 'DE_tennet_solar_generation_actual_scale': StandardScaler(), 'DE_tennet_wind_generation_actual_center': StandardScaler(), 'DE_tennet_wind_generation_actual_scale': StandardScaler(), 'DE_tennet_wind_offshore_generation_actual_center': StandardScaler(), 'DE_tennet_wind_offshore_generation_actual_scale': StandardScaler(), 'DE_tennet_wind_onshore_generation_actual_center': StandardScaler(), 'DE_tennet_wind_onshore_generation_actual_scale': StandardScaler(), 'DE_transnetbw_load_actual_entsoe_transparency_center': StandardScaler(), 'DE_transnetbw_load_actual_entsoe_transparency_scale': StandardScaler(), 'DE_transnetbw_solar_generation_actual_center': StandardScaler(), 'DE_transnetbw_solar_generation_actual_scale': StandardScaler(), 'DE_transnetbw_wind_onshore_generation_actual_center': StandardScaler(), 'DE_transnetbw_wind_onshore_generation_actual_scale': StandardScaler(), 'time_idx': StandardScaler(), 'relative_time_idx': StandardScaler()},\n",
      "\trandomize_length=None,\n",
      "\tpredict_mode=False\n",
      ")\n",
      "TimeSeriesDataSet[length=4189](\n",
      "\ttime_idx='time_idx',\n",
      "\ttarget=['DE_load_actual_entsoe_transparency', 'DE_solar_capacity', 'DE_solar_generation_actual', 'DE_solar_profile', 'DE_wind_capacity', 'DE_wind_generation_actual', 'DE_wind_profile', 'DE_wind_offshore_capacity', 'DE_wind_offshore_generation_actual', 'DE_wind_offshore_profile', 'DE_wind_onshore_capacity', 'DE_wind_onshore_generation_actual', 'DE_wind_onshore_profile', 'DE_50hertz_load_actual_entsoe_transparency', 'DE_50hertz_solar_generation_actual', 'DE_50hertz_wind_generation_actual', 'DE_50hertz_wind_offshore_generation_actual', 'DE_50hertz_wind_onshore_generation_actual', 'DE_amprion_load_actual_entsoe_transparency', 'DE_amprion_solar_generation_actual', 'DE_amprion_wind_onshore_generation_actual', 'DE_tennet_load_actual_entsoe_transparency', 'DE_tennet_solar_generation_actual', 'DE_tennet_wind_generation_actual', 'DE_tennet_wind_offshore_generation_actual', 'DE_tennet_wind_onshore_generation_actual', 'DE_transnetbw_load_actual_entsoe_transparency', 'DE_transnetbw_solar_generation_actual', 'DE_transnetbw_wind_onshore_generation_actual'],\n",
      "\tgroup_ids=['DE'],\n",
      "\tweight=None,\n",
      "\tmax_encoder_length=96,\n",
      "\tmin_encoder_length=96,\n",
      "\tmin_prediction_idx=30655,\n",
      "\tmin_prediction_length=192,\n",
      "\tmax_prediction_length=192,\n",
      "\tstatic_categoricals=['DE'],\n",
      "\tstatic_reals=['encoder_length', 'DE_load_actual_entsoe_transparency_center', 'DE_load_actual_entsoe_transparency_scale', 'DE_solar_capacity_center', 'DE_solar_capacity_scale', 'DE_solar_generation_actual_center', 'DE_solar_generation_actual_scale', 'DE_solar_profile_center', 'DE_solar_profile_scale', 'DE_wind_capacity_center', 'DE_wind_capacity_scale', 'DE_wind_generation_actual_center', 'DE_wind_generation_actual_scale', 'DE_wind_profile_center', 'DE_wind_profile_scale', 'DE_wind_offshore_capacity_center', 'DE_wind_offshore_capacity_scale', 'DE_wind_offshore_generation_actual_center', 'DE_wind_offshore_generation_actual_scale', 'DE_wind_offshore_profile_center', 'DE_wind_offshore_profile_scale', 'DE_wind_onshore_capacity_center', 'DE_wind_onshore_capacity_scale', 'DE_wind_onshore_generation_actual_center', 'DE_wind_onshore_generation_actual_scale', 'DE_wind_onshore_profile_center', 'DE_wind_onshore_profile_scale', 'DE_50hertz_load_actual_entsoe_transparency_center', 'DE_50hertz_load_actual_entsoe_transparency_scale', 'DE_50hertz_solar_generation_actual_center', 'DE_50hertz_solar_generation_actual_scale', 'DE_50hertz_wind_generation_actual_center', 'DE_50hertz_wind_generation_actual_scale', 'DE_50hertz_wind_offshore_generation_actual_center', 'DE_50hertz_wind_offshore_generation_actual_scale', 'DE_50hertz_wind_onshore_generation_actual_center', 'DE_50hertz_wind_onshore_generation_actual_scale', 'DE_amprion_load_actual_entsoe_transparency_center', 'DE_amprion_load_actual_entsoe_transparency_scale', 'DE_amprion_solar_generation_actual_center', 'DE_amprion_solar_generation_actual_scale', 'DE_amprion_wind_onshore_generation_actual_center', 'DE_amprion_wind_onshore_generation_actual_scale', 'DE_tennet_load_actual_entsoe_transparency_center', 'DE_tennet_load_actual_entsoe_transparency_scale', 'DE_tennet_solar_generation_actual_center', 'DE_tennet_solar_generation_actual_scale', 'DE_tennet_wind_generation_actual_center', 'DE_tennet_wind_generation_actual_scale', 'DE_tennet_wind_offshore_generation_actual_center', 'DE_tennet_wind_offshore_generation_actual_scale', 'DE_tennet_wind_onshore_generation_actual_center', 'DE_tennet_wind_onshore_generation_actual_scale', 'DE_transnetbw_load_actual_entsoe_transparency_center', 'DE_transnetbw_load_actual_entsoe_transparency_scale', 'DE_transnetbw_solar_generation_actual_center', 'DE_transnetbw_solar_generation_actual_scale', 'DE_transnetbw_wind_onshore_generation_actual_center', 'DE_transnetbw_wind_onshore_generation_actual_scale'],\n",
      "\ttime_varying_known_categoricals=['hour_of_day', 'day_of_month', 'day_of_year', 'month_of_year', 'week_of_year', 'day_of_week'],\n",
      "\ttime_varying_known_reals=['time_idx', 'relative_time_idx'],\n",
      "\ttime_varying_unknown_categoricals=[],\n",
      "\ttime_varying_unknown_reals=['DE_load_actual_entsoe_transparency', 'DE_solar_capacity', 'DE_solar_generation_actual', 'DE_solar_profile', 'DE_wind_capacity', 'DE_wind_generation_actual', 'DE_wind_profile', 'DE_wind_offshore_capacity', 'DE_wind_offshore_generation_actual', 'DE_wind_offshore_profile', 'DE_wind_onshore_capacity', 'DE_wind_onshore_generation_actual', 'DE_wind_onshore_profile', 'DE_50hertz_load_actual_entsoe_transparency', 'DE_50hertz_solar_generation_actual', 'DE_50hertz_wind_generation_actual', 'DE_50hertz_wind_offshore_generation_actual', 'DE_50hertz_wind_onshore_generation_actual', 'DE_amprion_load_actual_entsoe_transparency', 'DE_amprion_solar_generation_actual', 'DE_amprion_wind_onshore_generation_actual', 'DE_tennet_load_actual_entsoe_transparency', 'DE_tennet_solar_generation_actual', 'DE_tennet_wind_generation_actual', 'DE_tennet_wind_offshore_generation_actual', 'DE_tennet_wind_onshore_generation_actual', 'DE_transnetbw_load_actual_entsoe_transparency', 'DE_transnetbw_solar_generation_actual', 'DE_transnetbw_wind_onshore_generation_actual'],\n",
      "\tvariable_groups={},\n",
      "\tconstant_fill_strategy={},\n",
      "\tallow_missing_timesteps=False,\n",
      "\tlags={},\n",
      "\tadd_relative_time_idx=True,\n",
      "\tadd_target_scales=True,\n",
      "\tadd_encoder_length=True,\n",
      "\ttarget_normalizer=MultiNormalizer(\n",
      "\tnormalizers=[TorchNormalizer(method='identity', center=False, transformation=None, method_kwargs={}), TorchNormalizer(method='identity', center=False, transformation=None, method_kwargs={}), TorchNormalizer(method='identity', center=False, transformation=None, method_kwargs={}), TorchNormalizer(method='identity', center=False, transformation=None, method_kwargs={}), TorchNormalizer(method='identity', center=False, transformation=None, method_kwargs={}), TorchNormalizer(method='identity', center=False, transformation=None, method_kwargs={}), TorchNormalizer(method='identity', center=False, transformation=None, method_kwargs={}), TorchNormalizer(method='identity', center=False, transformation=None, method_kwargs={}), TorchNormalizer(method='identity', center=False, transformation=None, method_kwargs={}), TorchNormalizer(method='identity', center=False, transformation=None, method_kwargs={}), TorchNormalizer(method='identity', center=False, transformation=None, method_kwargs={}), TorchNormalizer(method='identity', center=False, transformation=None, method_kwargs={}), TorchNormalizer(method='identity', center=False, transformation=None, method_kwargs={}), TorchNormalizer(method='identity', center=False, transformation=None, method_kwargs={}), TorchNormalizer(method='identity', center=False, transformation=None, method_kwargs={}), TorchNormalizer(method='identity', center=False, transformation=None, method_kwargs={}), TorchNormalizer(method='identity', center=False, transformation=None, method_kwargs={}), TorchNormalizer(method='identity', center=False, transformation=None, method_kwargs={}), TorchNormalizer(method='identity', center=False, transformation=None, method_kwargs={}), TorchNormalizer(method='identity', center=False, transformation=None, method_kwargs={}), TorchNormalizer(method='identity', center=False, transformation=None, method_kwargs={}), TorchNormalizer(method='identity', center=False, transformation=None, method_kwargs={}), TorchNormalizer(method='identity', center=False, transformation=None, method_kwargs={}), TorchNormalizer(method='identity', center=False, transformation=None, method_kwargs={}), TorchNormalizer(method='identity', center=False, transformation=None, method_kwargs={}), TorchNormalizer(method='identity', center=False, transformation=None, method_kwargs={}), TorchNormalizer(method='identity', center=False, transformation=None, method_kwargs={}), TorchNormalizer(method='identity', center=False, transformation=None, method_kwargs={}), TorchNormalizer(method='identity', center=False, transformation=None, method_kwargs={})]\n",
      "),\n",
      "\tcategorical_encoders={'__group_id__DE': NaNLabelEncoder(add_nan=False, warn=True), 'DE': NaNLabelEncoder(add_nan=False, warn=True), 'hour_of_day': NaNLabelEncoder(add_nan=False, warn=True), 'day_of_month': NaNLabelEncoder(add_nan=False, warn=True), 'day_of_year': NaNLabelEncoder(add_nan=False, warn=True), 'month_of_year': NaNLabelEncoder(add_nan=False, warn=True), 'week_of_year': NaNLabelEncoder(add_nan=False, warn=True), 'day_of_week': NaNLabelEncoder(add_nan=False, warn=True)},\n",
      "\tscalers={'encoder_length': StandardScaler(), 'DE_load_actual_entsoe_transparency_center': StandardScaler(), 'DE_load_actual_entsoe_transparency_scale': StandardScaler(), 'DE_solar_capacity_center': StandardScaler(), 'DE_solar_capacity_scale': StandardScaler(), 'DE_solar_generation_actual_center': StandardScaler(), 'DE_solar_generation_actual_scale': StandardScaler(), 'DE_solar_profile_center': StandardScaler(), 'DE_solar_profile_scale': StandardScaler(), 'DE_wind_capacity_center': StandardScaler(), 'DE_wind_capacity_scale': StandardScaler(), 'DE_wind_generation_actual_center': StandardScaler(), 'DE_wind_generation_actual_scale': StandardScaler(), 'DE_wind_profile_center': StandardScaler(), 'DE_wind_profile_scale': StandardScaler(), 'DE_wind_offshore_capacity_center': StandardScaler(), 'DE_wind_offshore_capacity_scale': StandardScaler(), 'DE_wind_offshore_generation_actual_center': StandardScaler(), 'DE_wind_offshore_generation_actual_scale': StandardScaler(), 'DE_wind_offshore_profile_center': StandardScaler(), 'DE_wind_offshore_profile_scale': StandardScaler(), 'DE_wind_onshore_capacity_center': StandardScaler(), 'DE_wind_onshore_capacity_scale': StandardScaler(), 'DE_wind_onshore_generation_actual_center': StandardScaler(), 'DE_wind_onshore_generation_actual_scale': StandardScaler(), 'DE_wind_onshore_profile_center': StandardScaler(), 'DE_wind_onshore_profile_scale': StandardScaler(), 'DE_50hertz_load_actual_entsoe_transparency_center': StandardScaler(), 'DE_50hertz_load_actual_entsoe_transparency_scale': StandardScaler(), 'DE_50hertz_solar_generation_actual_center': StandardScaler(), 'DE_50hertz_solar_generation_actual_scale': StandardScaler(), 'DE_50hertz_wind_generation_actual_center': StandardScaler(), 'DE_50hertz_wind_generation_actual_scale': StandardScaler(), 'DE_50hertz_wind_offshore_generation_actual_center': StandardScaler(), 'DE_50hertz_wind_offshore_generation_actual_scale': StandardScaler(), 'DE_50hertz_wind_onshore_generation_actual_center': StandardScaler(), 'DE_50hertz_wind_onshore_generation_actual_scale': StandardScaler(), 'DE_amprion_load_actual_entsoe_transparency_center': StandardScaler(), 'DE_amprion_load_actual_entsoe_transparency_scale': StandardScaler(), 'DE_amprion_solar_generation_actual_center': StandardScaler(), 'DE_amprion_solar_generation_actual_scale': StandardScaler(), 'DE_amprion_wind_onshore_generation_actual_center': StandardScaler(), 'DE_amprion_wind_onshore_generation_actual_scale': StandardScaler(), 'DE_tennet_load_actual_entsoe_transparency_center': StandardScaler(), 'DE_tennet_load_actual_entsoe_transparency_scale': StandardScaler(), 'DE_tennet_solar_generation_actual_center': StandardScaler(), 'DE_tennet_solar_generation_actual_scale': StandardScaler(), 'DE_tennet_wind_generation_actual_center': StandardScaler(), 'DE_tennet_wind_generation_actual_scale': StandardScaler(), 'DE_tennet_wind_offshore_generation_actual_center': StandardScaler(), 'DE_tennet_wind_offshore_generation_actual_scale': StandardScaler(), 'DE_tennet_wind_onshore_generation_actual_center': StandardScaler(), 'DE_tennet_wind_onshore_generation_actual_scale': StandardScaler(), 'DE_transnetbw_load_actual_entsoe_transparency_center': StandardScaler(), 'DE_transnetbw_load_actual_entsoe_transparency_scale': StandardScaler(), 'DE_transnetbw_solar_generation_actual_center': StandardScaler(), 'DE_transnetbw_solar_generation_actual_scale': StandardScaler(), 'DE_transnetbw_wind_onshore_generation_actual_center': StandardScaler(), 'DE_transnetbw_wind_onshore_generation_actual_scale': StandardScaler(), 'time_idx': StandardScaler(), 'relative_time_idx': StandardScaler()},\n",
      "\trandomize_length=None,\n",
      "\tpredict_mode=False\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 3 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=3)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in network: 89.0k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | MultiLoss                       | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 7.3 K \n",
      "3  | prescalers                         | ModuleDict                      | 1.4 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 41.4 K\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 23.6 K\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 2.4 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K \n",
      "11 | lstm_encoder                       | LSTM                            | 2.2 K \n",
      "12 | lstm_decoder                       | LSTM                            | 2.2 K \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544   \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 32    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K \n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 808   \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 576   \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 576   \n",
      "20 | output_layer                       | ModuleList                      | 493   \n",
      "----------------------------------------------------------------------------------------\n",
      "89.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "89.0 K    Total params\n",
      "0.356     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 50/50 [01:31<00:00,  0.54it/s, v_num=56, train_loss_step=12.80]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|██▌       | 1/4 [02:07<06:21,  0.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 2/4 [02:15<02:15,  0.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████▌  | 3/4 [02:25<00:48,  0.02it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 4/4 [02:28<00:00,  0.03it/s]\u001b[A\n",
      "Epoch 1: 100%|██████████| 50/50 [01:31<00:00,  0.54it/s, v_num=56, train_loss_step=9.780, val_loss=20.30, train_loss_epoch=17.00]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|██▌       | 1/4 [02:00<06:02,  0.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 2/4 [02:09<02:09,  0.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████▌  | 3/4 [02:18<00:46,  0.02it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 4/4 [02:21<00:00,  0.03it/s]\u001b[A\n",
      "Epoch 2: 100%|██████████| 50/50 [01:31<00:00,  0.54it/s, v_num=56, train_loss_step=8.530, val_loss=20.90, train_loss_epoch=11.00]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|██▌       | 1/4 [02:07<06:23,  0.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 2/4 [02:16<02:16,  0.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████▌  | 3/4 [02:26<00:48,  0.02it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 4/4 [02:29<00:00,  0.03it/s]\u001b[A\n",
      "Epoch 3: 100%|██████████| 50/50 [01:32<00:00,  0.54it/s, v_num=56, train_loss_step=8.010, val_loss=20.70, train_loss_epoch=9.180]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|██▌       | 1/4 [01:59<05:57,  0.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 2/4 [02:08<02:08,  0.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████▌  | 3/4 [02:17<00:45,  0.02it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 4/4 [02:20<00:00,  0.03it/s]\u001b[A\n",
      "Epoch 4: 100%|██████████| 50/50 [01:31<00:00,  0.55it/s, v_num=56, train_loss_step=7.590, val_loss=20.40, train_loss_epoch=8.260]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|██▌       | 1/4 [02:05<06:17,  0.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 2/4 [02:15<02:15,  0.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████▌  | 3/4 [02:24<00:48,  0.02it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 4/4 [02:27<00:00,  0.03it/s]\u001b[A\n",
      "Epoch 5: 100%|██████████| 50/50 [01:32<00:00,  0.54it/s, v_num=56, train_loss_step=7.110, val_loss=20.70, train_loss_epoch=7.840]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|██▌       | 1/4 [02:04<06:12,  0.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 2/4 [02:13<02:13,  0.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████▌  | 3/4 [02:22<00:47,  0.02it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 4/4 [02:25<00:00,  0.03it/s]\u001b[A\n",
      "Epoch 6: 100%|██████████| 50/50 [01:32<00:00,  0.54it/s, v_num=56, train_loss_step=6.900, val_loss=21.30, train_loss_epoch=7.340]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|██▌       | 1/4 [02:06<06:18,  0.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 2/4 [02:14<02:14,  0.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████▌  | 3/4 [02:23<00:47,  0.02it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 4/4 [02:26<00:00,  0.03it/s]\u001b[A\n",
      "Epoch 7: 100%|██████████| 50/50 [01:33<00:00,  0.53it/s, v_num=56, train_loss_step=6.730, val_loss=20.90, train_loss_epoch=6.970]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|██▌       | 1/4 [02:04<06:13,  0.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 2/4 [02:14<02:14,  0.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████▌  | 3/4 [02:23<00:47,  0.02it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 4/4 [02:26<00:00,  0.03it/s]\u001b[A\n",
      "Epoch 8: 100%|██████████| 50/50 [01:32<00:00,  0.54it/s, v_num=56, train_loss_step=6.460, val_loss=21.20, train_loss_epoch=6.710]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|██▌       | 1/4 [02:06<06:20,  0.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 2/4 [02:15<02:15,  0.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████▌  | 3/4 [02:24<00:48,  0.02it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 4/4 [02:27<00:00,  0.03it/s]\u001b[A\n",
      "Epoch 9: 100%|██████████| 50/50 [01:33<00:00,  0.53it/s, v_num=56, train_loss_step=6.320, val_loss=21.20, train_loss_epoch=6.610]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|██▌       | 1/4 [02:04<06:12,  0.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 2/4 [02:13<02:13,  0.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████▌  | 3/4 [02:22<00:47,  0.02it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 4/4 [02:25<00:00,  0.03it/s]\u001b[A\n",
      "Epoch 10: 100%|██████████| 50/50 [01:32<00:00,  0.54it/s, v_num=56, train_loss_step=6.130, val_loss=21.20, train_loss_epoch=6.480]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|██▌       | 1/4 [02:01<06:04,  0.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 2/4 [02:10<02:10,  0.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████▌  | 3/4 [02:20<00:46,  0.02it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 4/4 [02:23<00:00,  0.03it/s]\u001b[A\n",
      "Epoch 10: 100%|██████████| 50/50 [04:00<00:00,  0.21it/s, v_num=56, train_loss_step=6.130, val_loss=21.60, train_loss_epoch=6.340]\n",
      "Model information saved to TFT/model_info_multivariate_2.txt\n",
      "192\n"
     ]
    }
   ],
   "source": [
    "experiment_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88bc5ba-da95-49e3-8263-71df16e361a9",
   "metadata": {},
   "source": [
    "## 5. Calculate the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a29be7-7fc8-4428-8749-fe5ace6d56d9",
   "metadata": {},
   "source": [
    "### Prepare test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0f4167e6-860a-4697-b4c7-fcfef3159788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First before comparing the predictions to the actuals we need to feed a modified version of our test dataset to the modelto get predictions\n",
    "# In the following part some parameters should be manually adjusted as needed depending on the target\n",
    "test_dataset = test_data_all_columns #adjust as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9f0caedf-84f5-4e42-ad11-04e43b1793df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the same time features so we can use them as future known categoricals\n",
    "test_dataset = add_time_features(test_dataset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d46fe845-f79f-4fb3-a5df-d635a54a5067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed because TFT needs a group column\n",
    "test_dataset['DE'] = 'DE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1f8218c2-6715-4fbd-862a-8b577b9f2c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is pytorch-forecasting syntax specific, the time_idx index should be continous so we set it continuing the time_idx index from train+val\n",
    "test_dataset['time_idx'] = range(30536, 30536 + len(test_dataset)) #this is pytorch-forecasting syntax specific, the time_idx index should be continous so we set it continuing the time_idx index from train+val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "57277956-62d9-4ca4-baf3-fc58f45d37c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the data index and delete this column\n",
    "test_dataset=test_dataset.reset_index()\n",
    "test_dataset=test_dataset.drop(columns='date')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd43fd25-a84d-4b93-a67d-56e59f5026c3",
   "metadata": {},
   "source": [
    "### Using the test dataset to feed information to the encoder and decoder in a rolling way to get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d6aac1de-f056-4684-867a-21b8cae6d616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the columns to know the targets\n",
    "columns = [col for col in test_dataset.columns if col not in ['DE', 'time_idx','hour_of_day','day_of_month','day_of_year','month_of_year','week_of_year','day_of_week']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012a6873-b81d-4b85-ad21-692d3cc3d569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint('lightning_logs/lightning_logs/version_53/checkpoints/epoch=12-step=650.ckpt')# adjust as needed\n",
    "# Define an empty dataframe to append all predictions to\n",
    "all_df = pd.DataFrame()\n",
    "\n",
    "max_encoder_length = 96  \n",
    "max_prediction_length = 48 # adjust as needed\n",
    "target_name = columns\n",
    "\n",
    "# We need to stop our predictions max_encoder_length + max_prediction_length before the end of the test dataset otherwise we get errors\n",
    "addition_var = max_encoder_length + max_prediction_length\n",
    "\n",
    "for i in range(0, len(test_dataset)-addition_var, max_prediction_length):\n",
    "    new_pred_collection = []\n",
    "    # Define the data for the encoder and decoder\n",
    "    # The encoder contains data from i until the max_encoder_length\n",
    "    test_dataset_short = test_dataset.iloc[i : i + max_encoder_length]\n",
    "    encoder_data = test_dataset_short[lambda x: x.time_idx > x.time_idx.max() - max_encoder_length]\n",
    "\n",
    "    # The decoder contains information (here: known future time features) from the last point of the encoder until the prediction length\n",
    "    test_dataset_decoder = test_dataset.iloc[i+ max_encoder_length : i + max_encoder_length + max_prediction_length]\n",
    "\n",
    "    # Set the targets to zero so the model is not spoiled\n",
    "    for i in columns:\n",
    "        test_dataset_decoder[i]=0.0\n",
    "    decoder_data = test_dataset_decoder\n",
    "    \n",
    "    # Concatenate encoder and decoder information\n",
    "    new_prediction_data = pd.concat([encoder_data, decoder_data], ignore_index=True)\n",
    "    \n",
    "    # Make predictions\n",
    "    new_raw_predictions = best_tft.predict(new_prediction_data, mode=\"raw\", return_x=True, return_index = True)\n",
    "  \n",
    "    # Reformatting and writing the current predictions in the overall all_df\n",
    "    for i in range (test_dataset.shape[1]-8):\n",
    "        new_pred = (new_raw_predictions[0][0][i].cpu().detach().numpy())\n",
    "        new_pred = new_pred.squeeze()\n",
    "        new_pred_collection.append(new_pred)\n",
    "        \n",
    "    local_df = pd.DataFrame(new_pred_collection).transpose()\n",
    "    all_df = pd.concat([all_df, local_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5344aa32-aee4-48b2-a103-e599617f069a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Calculate MAE and MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "39113b80-82f3-4c22-899b-42f16ba90c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The original test_dataset and the all_df with all our predictions do not have the same size yet\n",
    "# This is for two main reasons 1) The predictions only start after one full length of encoder input of size (96), therefore we cut off the first 96 rows\n",
    "# Additionally since we stop max_encoder_length + max_prediction_length steps before the end of test dataset due to pytorch-forecasting TFT implementation,\n",
    "# we need to stop early and cut-off in the end as well \n",
    "\n",
    "def calc_mse_mae(test_dataset, all_df):\n",
    "    columns = [col for col in test_dataset.columns if col not in ['DE', 'time_idx','hour_of_day','day_of_month','day_of_year','month_of_year','week_of_year','day_of_week']]\n",
    "    # Cut off at the start\n",
    "    test_dataset_2=test_dataset[96:]\n",
    "\n",
    "    # Cutting at the end\n",
    "    length_all_df = len(all_df)\n",
    "    length_test_dataset_2 = len(test_dataset_2)\n",
    "    difference_to_substract = length_test_dataset_2-length_all_df\n",
    "    test_dataset_2 = test_dataset_2.iloc[:-difference_to_substract] \n",
    "    # Only keep the targets in the test dataset\n",
    "    test_dataset_2= test_dataset_2[columns]\n",
    "  \n",
    "    all_df_tensor = torch.tensor(all_df.values)\n",
    "    torch_test_dataset = torch.tensor(test_dataset_2.values)\n",
    "\n",
    "    print(torch_test_dataset.size())\n",
    "    print(all_df_tensor.size())\n",
    "\n",
    "    mae = F.l1_loss(all_df_tensor, torch_test_dataset)\n",
    "    # Print the MAE\n",
    "    print(\"MAE:\", mae)\n",
    "\n",
    "    # Calculate the MSE\n",
    "    mse = F.mse_loss(all_df_tensor, torch_test_dataset)\n",
    "    # Print the MSE\n",
    "    print(\"MSE:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4619ed63-bb6f-4aee-b648-8f8d067854a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8640, 29])\n",
      "torch.Size([8640, 29])\n",
      "MAE: tensor(0.9574, dtype=torch.float64)\n",
      "MSE: tensor(2.0916, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "calc_mse_mae(test_dataset, all_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
